<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="">
  <meta property="og:title" content="GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models"/>
  <meta property="og:description" content=""/>
  <meta property="og:url" content="https://jmiemirza.github.io/GLOV/"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="">
  <meta name="twitter:description" content="">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Meta-Prompting</title>
  <link rel="icon" type="image/x-icon" href="static/images/car.png">
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <!-- <span class="author-block">
                <a href="https://jmiemirza.github.io/" target="_blank">M. Jehanzeb Mirza</a><sup>1,2</sup>,</span>
                <span class="author-block">
                  <a href="https://mitibmwatsonailab.mit.edu/people/leonid-karlinsky/" target="_blank">Leonid Karlinsky</a><sup>3</sup>,</span>
                  <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=JJRr8c8AAAAJ&hl=en" target="_blank">Wei Lin</a><sup>4</sup>,
                  </span>
                            <span class="author-block">
                    <a href="https://scholar.google.com.hk/citations?user=ER4dt8cAAAAJ&hl=ja" target="_blank">Sivan Doveh</a><sup>5,6</sup>
                  </span>

              </span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=hFcNhWEAAAAJ&hl=en" target="_blank">Jakub Micorek</a><sup>1</sup>
                  </span>

              </span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=oDDqnQ4AAAAJ&hl=en" target="_blank">Mateusz Kozinski</a><sup>1</sup>,

                </span>

              <span class="author-block">
                    <a href="https://hildekuehne.github.io/" target="_blank">Hilde Kuhene</a><sup>3,7</sup>
                  </span>

                  </span>
              <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=iWPrl3wAAAAJ&hl=en" target="_blank">Horst Possegger</a><sup>1</sup>
                  </span> -->
                  <span class="author-block">
                    <a href="https://jmiemirza.github.io/" target="_blank">M. Jehanzeb Mirza</a><sup>1</sup>,
                </span>
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=G6ema1YAAAAJ&hl=en" target="_blank">Mengjie Zhao</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="https://scholar.google.co.jp/citations?user=gIzJ2sQAAAAJ&hl=ja" target="_blank">Zhuoyuan Mao</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="https://scholar.google.co.il/citations?user=ER4dt8cAAAAJ&hl=iw" target="_blank">Sivan Doveh</a><sup>3</sup>,
                </span>
                <span class="author-block">
                    <a href="https://wlin-at.github.io/" target="_blank">Wei Lin</a><sup>4</sup>,
                </span>
                <span class="author-block">
                    <a href="https://paulgavrikov.github.io/" target="_blank">Paul Gavrikov</a><sup>5</sup>,
                </span>
                <span class="author-block">
                    <a href="https://mdorkenwald.com/" target="_blank">Michael Dorkenwald</a><sup>6</sup>,
                </span>
                <span class="author-block">
                    <a href="https://www.shiqiyang.xyz/" target="_blank">Shiqi Yang</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="http://sauravjha.com.np/" target="_blank">Saurav Jha</a><sup>7</sup>,
                </span>
                <span class="author-block">
                    <a href="https://scholar.google.co.jp/citations?user=lv41luwAAAAJ&hl=ja" target="_blank">Hiromi Wakaki</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="https://scholar.google.com/citations?user=GMytI10AAAAJ&hl=en" target="_blank">Yuki Mitsufuji</a><sup>2,8</sup>,
                </span>
                <span class="author-block">
                    <a href="https://snototter.github.io/research/" target="_blank">Horst Possegger</a><sup>2</sup>,
                </span>
                <span class="author-block">
                    <a href="https://www.rogerioferis.org/" target="_blank">Rogerio Feris</a><sup>9</sup>,
                </span>
                <span class="author-block">
                    <a href="https://mitibmwatsonailab.mit.edu/people/leonid-karlinsky/" target="_blank">Leonid Karlinsky</a><sup>9</sup>,
                </span>
                <span class="author-block">
                    <a href="https://www.csail.mit.edu/person/jim-glass" target="_blank">James Glass</a><sup>10</sup>
                </span>
                  <br>
                  </div>

                  <div class="is-size-5 publication-authors">
                    <span class="author-block"><sup>1</sup>ICG, TU Graz, Austria. <sup>2</sup>Sony Group Corporation, Japan. <sup>3</sup>IBM Research, Israel. <sup>4</sup>JKU Linz, Austria. <sup>5</sup>Offenburg University, Germany.
                      <sup>6</sup>University of Amsterdam, Netherlands. <sup>7</sup>UNSW Sydney, Australia. <sup>8</sup>SonyAI, USA. <sup>9</sup>MIT-IBM Watson AI Lab, USA. <sup>10</sup>MIT CSAIL, USA.
                        <br>ArXiv Preprint</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

<!--                    &lt;!&ndash; Supplementary PDF link &ndash;&gt;-->
<!--                    <span class="link-block">-->
<!--                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"-->
<!--                      class="external-link button is-normal is-rounded is-dark">-->
<!--                      <span class="icon">-->
<!--                        <i class="fas fa-file-pdf"></i>-->
<!--                      </span>-->
<!--                      <span>Supplementary</span>-->
<!--                    </a>-->
<!--                  </span>-->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/jmiemirza/GLOV" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link -->
<!--                <span class="link-block">-->
<!--                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"-->
<!--                  class="external-link button is-normal is-rounded is-dark">-->
<!--                  <span class="icon">-->
<!--                    <i class="ai ai-arxiv"></i>-->
<!--                  </span>-->
<!--                  <span>Abstract</span>-->
<!--                </a>-->
<!--              </span>-->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <img src="static/images/teaser.jpeg" alt="Image Description" height="100%">
      <h2 class="subtitle has-text-justified is-size-6">
        The effect of prompt evolution on the downstream task performance. 
        The shaded regions represent the absolute top-1 classification accuracies for ImageNet at each optimization step by ensembling the top-3 prompts found w.r.t the accuracy on the 1-shot train set whereas the solid lines represent the exponential moving average. 
        The left plot is with CLIP VIT-B/32, and the right is with LlaVA-OV while the LLM employed is Llama-3. Due to high computational cost, we only perform 25 optimization steps for LlaVA-OV.
      </h2>
    </div>
  </div>
</section>

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            In this work, we propose a novel method (GLOV) enabling Large Language Models (LLMs) to act as implicit Optimizers for Vision-Language Models (VLMs) to enhance downstream vision tasks. 
Our GLOV meta-prompts an LLM with the downstream task description, querying it for suitable VLM prompts (e.g., for zero-shot classification with CLIP). 
These prompts are ranked according to their fitness for the downstream vision task. 
In each respective optimization step, the ranked prompts are fed as in-context examples (with their accuracies) to equip the LLM with the knowledge of the type of 
prompts preferred by the downstream VLM. 
Furthermore, we also explicitly steer the LLM generation in each optimization step 
by specifically adding an offset difference vector of the embeddings from the positive and negative solutions found by the LLM, in previous optimization steps, to the intermediate layer of the network for the next generation step.
This offset vector steers the LLM generation toward the type of language preferred by the downstream VLM, resulting in enhanced performance on the downstream vision tasks. 
We comprehensively evaluate our GLOV on 16 diverse datasets using two families of VLMs, i.e., dual-encoder (e.g., CLIP) and encoder-decoder (e.g., LLaVa) models --
showing that the discovered solutions can enhance the recognition performance by up to 15.0% and 57.5% (3.8% and 21.6% on average) for these models. 
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel
<section class="hero is-small">
  <div class="hero-body">
    <div class="container">
                  <h2 class="title is-3">Method and Results</h2>
    <div class="level-set has-text-justified">
                <p>
                    Given a pre-trained model and statistics of the clean activations from the training data, our ActMAD aligns the
activation responses from the shifted test data to the clean activations at test-time. We model the activation distributions in terms of the means
and variances of each activation, such that the statistics have the same shape as the feature maps. The statistics of the training activations are
pre-computed on the training set, or computed on unlabelled data without distribution shift.
                </p>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
         <img src="static/images/method.jpg" alt="MY ALT TEXT" style="width:85%;"/>

        <h2 class="subtitle has-text-centered">
          Schematic of ActMAD.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/teaser.jpg" alt="MY ALT TEXT"  style="width:85%;"/>
        <h2 class="subtitle has-text-centered">
          Continuous online adaptation for ActMAD in changing weather conditions.
        </h2> -->

        <section class="hero is-small">
          <div class="container is-max-desktop">
            <div class="hero-body">
                                  <h2 class="title is-3">Method</h2>
        
              <img src="static/images/main_.png" alt="Image Description" height="50%">
              <h2 class="subtitle has-text-justified is-size-6">
                Overview of GLOV. 
                GLOV consists of a Meta-Prompt, which constitutes system instruction, 
                task description, and in-context examples (VLM prompts) which are evaluated (and ranked) 
                on a few-shot training data in each iteration. The Meta-Prompt instructs the LLM to generate several 
                candidate solutions in each optimization iteration, conditioned on the in-context examples which are 
                fed in conjunction with the accuracy values, highlighting their effectiveness. Furthermore, to steer 
                the LLM generation towards the language preferred by the VLM, we add the scaled difference of the sentence 
                embeddings (autoregressively) from the positive and negative text prompts to the 
                intermediate layer of the LLM. This process is repeated until the stopping condition is met (e.g., maximum iterations). 
                Note, that H<sub>+</sub> and H<sub>-</sub> refer to the sentence embeddings from the text prompts.
              </h2>
            </div>
          </div>
        </section>
<!-- <section class="hero is-small">
  <div class="container is-max-desktop">
    <div class="hero-body">
                          <h2 class="title is-3">Main Results</h2>

      <img src="static/images/main_results.png" alt="Image Description" height="20%">
      <h2 class="subtitle has-text-justified is-size-6">
        Top-1 accuracy (%) for 20 datasets obtained by employing the ViT-B/32 backbone from OpenAI CLIP. S-TEMP refer to the results obtained by using the default template (a photo of a class name), while DS-TEMP refer to the results obtained by using the ensemble of dataset specific prompts.
An empty placeholder for CUPL indicates that the respective baseline did not provide the handcrafted prompts for the dataset.
For Waffle, mean results from 7 random runs are reported, following the original publication.
      </h2>
    </div>
  </div>
</section> -->
<!-- Image carousel -->
<!-- <section class="hero is-small">
  <div class="hero-body">
    <div class="container">
                  <h2 class="title is-3">Results</h2>
    <div class="level-set has-text-justified">
                <p>
                    Given a pre-trained model and statistics of the clean activations from the training data, our ActMAD aligns the
activation responses from the shifted test data to the clean activations at test-time. We model the activation distributions in terms of the means
and variances of each activation, such that the statistics have the same shape as the feature maps. The statistics of the training activations are
pre-computed on the training set, or computed on unlabelled data without distribution shift.
                </p>
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
         <img src="static/images/method.jpg" alt="MY ALT TEXT" style="width:85%;"/>

        <h2 class="subtitle has-text-centered">
          Schematic of ActMAD.
        </h2>
      </div>
      <div class="item">
        <img src="static/images/teaser.jpg" alt="MY ALT TEXT"  style="width:85%;"/>
        <h2 class="subtitle has-text-centered">
          Continuous online adaptation for ActMAD in changing weather conditions.
        </h2> -->

        <!--      </div>-->
<!--      <div class="item">-->
<!--        &lt;!&ndash; Your image here &ndash;&gt;-->
<!--        <img src="static/images/carousel3.jpg" alt="MY ALT TEXT"/>-->
<!--        <h2 class="subtitle has-text-centered">-->
<!--         Third image description.-->
<!--       </h2>-->
<!--     </div>-->
<!--     <div class="item">-->
<!--      &lt;!&ndash; Your image here &ndash;&gt;-->
<!--      <img src="static/images/carousel4.jpg" alt="MY ALT TEXT"/>-->
<!--      <h2 class="subtitle has-text-centered">-->
<!--        Fourth image description.-->
<!--      </h2>-->
<!--    </div>-->
  </div>
</div>
</div>
<!--</section>-->
<!-- End image carousel-->




<!--&lt;!&ndash; Youtube video &ndash;&gt;-->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      &lt;!&ndash; Paper video. &ndash;&gt;-->
<!--      <h2 class="title is-3">Video Presentation</h2>-->
<!--      <div class="columns is-centered has-text-centered">-->
<!--        <div class="column is-four-fifths">-->
<!--          -->
<!--          <div class="publication-video">-->
<!--            &lt;!&ndash; Youtube embed code here &ndash;&gt;-->
<!--            <iframe src="https://www.youtube.com/embed/JkaxUblCGz0" frameborder="0" allow="autoplay; encrypted-media" allowfullscreen></iframe>-->
<!--          </div>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End youtube video &ndash;&gt;-->


<!--&lt;!&ndash; Video carousel &ndash;&gt;-->
<!--<section class="hero is-small">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title is-3">Another Carousel</h2>-->
<!--      <div id="results-carousel" class="carousel results-carousel">-->
<!--        <div class="item item-video1">-->
<!--          <video poster="" id="video1" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel1.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video2">-->
<!--          <video poster="" id="video2" autoplay controls muted loop height="100%">-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel2.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--        <div class="item item-video3">-->
<!--          <video poster="" id="video3" autoplay controls muted loop height="100%">\-->
<!--            &lt;!&ndash; Your video file here &ndash;&gt;-->
<!--            <source src="static/videos/carousel3.mp4"-->
<!--            type="video/mp4">-->
<!--          </video>-->
<!--        </div>-->
<!--      </div>-->
<!--    </div>-->
<!--  </div>-->
<!--</section>-->
<!--&lt;!&ndash; End video carousel &ndash;&gt;-->






<!-- Paper poster -->
<!--<section class="hero is-small is-light">-->
<!--  <div class="hero-body">-->
<!--    <div class="container">-->
<!--      <h2 class="title">Poster</h2>-->

<!--      <iframe  src="static/pdfs/sample.pdf" width="100%" height="550">-->
<!--          </iframe>-->

<!--      </div>-->
<!--    </div>-->
<!--  </section>-->
<!--End paper poster -->


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@article{mirza2024glov,
        author    = {Mirza, M. Jehanzeb and Zhao, Mengjie and Mao, Zhuoyuan and Doveh, Sivan and Lin, Wei and Gavrikov, Paul and Dorkenwald, Michael and Yang, Shiqi and Jha, Saurav and Wakaki, Hiromi and Mitsufuji, Yuki and Possegger, Horst and Feris, Rogerio and Karlinsky, Leonid and Glass, James},
        journal   = {ArXiv},
        title     = {GLOV: Guided Large Language Models as Implicit Optimizers for Vision Language Models},
        year      = {2024}
        }</code></pre>
    </div>
</section>
<!--End BibTex citation -->


  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            The website template has been shamelessly copied from: <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a>.
<!--            You are free to borrow the of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative-->
<!--            Commons Attribution-ShareAlike 4.0 International License</a>.-->
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
